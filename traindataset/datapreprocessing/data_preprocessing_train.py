# -*- coding: utf-8 -*-
"""Data Preprocessing Train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mnurNHk1xCJbclKJPnPrn_XYld9WLBK6

Importing necessary libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Mount Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Load the Datasets"""

train_path = "/content/drive/MyDrive/Data Science Life Cycle/traincsv.csv"
train_df = pd.read_csv(train_path)

print("Train Dataset")
display(train_df.head())

"""Display Basic Information"""

print("Train Dataset Info:")
train_df.info()

"""Checking for missing values"""

print("\nMissing Values in Train Data:")
print(train_df.isnull().sum())

"""Summary Statictics"""

print("\nSummary Statistics (Train Dataset):")
print(train_df.describe())

"""Visualizing Missing Values"""

plt.figure(figsize=(10, 5))
sns.heatmap(train_df.isnull(), cbar=False, cmap='viridis')
plt.title("Missing Values Before Imputation")
plt.show()

"""Convert sales columns to numeric(train dataset)"""

sales_cols = ['luxury_sales', 'fresh_sales', 'dry_sales']
train_df[sales_cols] = train_df[sales_cols].apply(pd.to_numeric, errors='coerce')

numeric_cols = train_df.drop(columns=['Customer_ID']).select_dtypes(include=['number']).columns

train_df[numeric_cols].hist(figsize=(12, 8), bins=20)
plt.suptitle("Feature Distributions Before Filling Missing Values")
plt.show()

"""Boxplots to Identify Outliers

Fill missing values for categorical columns with the mode
"""

train_df.loc[:, 'outlet_city'] = train_df['outlet_city'].fillna(train_df['outlet_city'].mode()[0])

"""
Fill missing values for numerical columns with the median"""

for col in sales_cols:
    train_df.loc[:, col] = train_df[col].fillna(train_df[col].median())

"""Drop rows where Customer_ID is missing"""

train_df = train_df.dropna(subset=['Customer_ID'])

"""Drop rows where the target variable is missing"""

train_df.dropna(subset=["cluster_catgeory"], inplace=True)

print("\nMissing Values Count After Filling for train dataset:")
print(train_df.isnull().sum())

"""Check for duplicate rows"""

print("Duplicate Rows in Train Data:", train_df.duplicated().sum())

"""Get all unique values in cluster_category (target variable)"""

print("\nUnique Cluster Categories:\n", train_df["cluster_catgeory"].unique())

print("Cluster Category Counts:\n", train_df["cluster_catgeory"].value_counts())

"""Fixing Cluster Category Inconsistencies"""

# Convert cluster category to integer to remove inconsistencies
train_df["cluster_catgeory"] = pd.to_numeric(train_df["cluster_catgeory"], errors="coerce").astype("Int64")

# Drop remaining rare clusters (89, 95, 98, 99, 100)
train_df = train_df[~train_df["cluster_catgeory"].isin([89, 95, 98, 99, 100])]

# Drop NaN values in cluster category
train_df = train_df.dropna(subset=["cluster_catgeory"])

# Convert back to categorical
train_df["cluster_catgeory"] = train_df["cluster_catgeory"].astype("category")

# Verify cleaned cluster categories
print("Final Unique Cluster Categories:", train_df["cluster_catgeory"].unique())
print("\nFinal Cluster Counts:\n", train_df["cluster_catgeory"].value_counts())

"""Check if Customer_IDs are unique"""

print("\nNumber of Unique Customer IDs in Train:", train_df["Customer_ID"].nunique(),
      "out of", len(train_df), "rows")

# Convert Customer_ID to string and remove any decimals
train_df["Customer_ID"] = train_df["Customer_ID"].astype("Int64")

# Check sample values
print("Cleaned Customer_ID sample:", train_df["Customer_ID"].sample(10).tolist())

"""Get all unique values in Outlet City"""

print("\nUnique Outlet Cities in Train Data:\n", train_df["outlet_city"].unique())

"""Standardize Outlet City Names"""

# Convert to lowercase for consistency
train_df["outlet_city"] = train_df["outlet_city"].str.lower()

# Verify unique values after fixing
print("Cleaned Unique Outlet Cities in Train Data:\n", train_df["outlet_city"].unique())

"""
Check for Negative Sales Values"""

sales_cols = ["luxury_sales", "fresh_sales", "dry_sales"]
for col in sales_cols:
    negative_values = train_df[train_df[col] < 0]
    print(f"\nNegative Values in {col}: {negative_values.shape[0]} rows")

print("Train Dataset Data Types:")
print(train_df.dtypes)

"""Convert outlet_city column to category"""

train_df['outlet_city'] = train_df['outlet_city'].astype('category')

print("\nUpdated Data Types:")
print(train_df.dtypes)

"""Remove Outliers Using IQR Method"""

numeric_cols = ['luxury_sales', 'fresh_sales', 'dry_sales']

def cap_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Cap values above the upper bound
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])

# Apply IQR capping to all sales columns
for col in ['luxury_sales', 'fresh_sales', 'dry_sales']:
    cap_outliers(train_df, col)

# Verify changes with a new boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(data=train_df[numeric_cols])
plt.title("Boxplot of Numeric Features (Train Data)")
plt.xticks(rotation=45)
plt.show()

"""Encoding"""

print("\nUnique outlet_city values in Train:", train_df['outlet_city'].nunique())
print(train_df['outlet_city'].value_counts())

"""Encoding using labelencoder"""

from sklearn.preprocessing import LabelEncoder
import pickle

# Initialize Label Encoder
le_outlet = LabelEncoder()

# Fit & transform on train data
train_df["outlet_city_encoded"] = le_outlet.fit_transform(train_df["outlet_city"])

# Save the encoder for consistency with test data
with open("label_encoder_outlet.pkl", "wb") as f:
    pickle.dump(le_outlet, f)

# Verify encoding
print("\nEncoded Outlet City in Train Data:")
print(dict(zip(le_outlet.classes_, le_outlet.transform(le_outlet.classes_))))

"""Save the processed dataset"""

output_path = "/content/drive/MyDrive/Data Science Life Cycle/processed_Train_data.csv"
train_df.to_csv(output_path, index=False)