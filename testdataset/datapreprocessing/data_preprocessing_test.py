# -*- coding: utf-8 -*-
"""Data Preprocessing Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p6IdcApVB-g2EskxyittDYu1oD16KMEu

Importing necessary libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Load the Datasets"""

test_path = '/content/test.csv'
test_df = pd.read_csv(test_path)

print("Test Dataset")
display(test_df.head())

"""Display Basic Information"""

print("\nTest Dataset Info:")
test_df.info()

"""Checking for missing values"""

print("\nMissing Values in Test Data:")
print(test_df.isnull().sum())

"""Summary Statictics"""

print("\nSummary Statistics (Test Dataset):")
print(test_df.describe())

"""Check for duplicate rows"""

print("Duplicate Rows in Test Data:", test_df.duplicated().sum())

# Check for non-numeric values in sales columns
sales_cols = ['luxury_sales', 'fresh_sales', 'dry_sales']

for col in sales_cols:
    print(f"Unique values in {col} that are non-numeric:")
    print(test_df[col][~test_df[col].astype(str).str.replace('.', '', regex=True).str.isnumeric()].unique())

# Convert sales columns to numeric, forcing errors to NaN
for col in sales_cols:
    test_df[col] = pd.to_numeric(test_df[col], errors="coerce")

# Check for NaNs introduced after conversion
print("Missing Values After Fixing Non-Numeric Sales Data:\n", test_df[sales_cols].isnull().sum())

# Fill missing sales values with the median of each column
for col in sales_cols:
    test_df[col] = test_df[col].fillna(test_df[col].median())

# Verify missing values again
print("Missing Values in Test Data After Filling:\n", test_df[sales_cols].isnull().sum())

"""Check if Customer_IDs are unique"""

print("Number of Unique Customer IDs in Test:", test_df["Customer_ID"].nunique(),
      "out of", len(test_df), "rows")

# Convert Customer_ID to string and remove any decimals
test_df["Customer_ID"] = test_df["Customer_ID"].astype(str).str.replace(".0", "", regex=False)

"""Get all unique values in Outlet City"""

print("\nUnique Outlet Cities in Test Data:\n", test_df["outlet_city"].unique())

"""Standardize Outlet City Names"""

# Convert to lowercase for consistency
test_df["outlet_city"] = test_df["outlet_city"].str.lower()

# Manual corrections for known typos
corrections = {
    "trincomale": "trincomalee",
    "peliyagoda": "peliyagoda",
    "peliyagodA": "peliyagoda",
    "moratuwa": "moratuwa",
    "MoraTuwa": "moratuwa",
    "kalmunai": "kalmunai"
}

# Apply corrections
test_df["outlet_city"] = test_df["outlet_city"].replace(corrections)

# Verify unique values after fixing
print("\nCleaned Unique Outlet Cities in Test Data:\n", test_df["outlet_city"].unique())

print("\nTest Dataset Data Types:")
print(test_df.dtypes)

"""Convert outlet_city column to category"""

test_df['outlet_city'] = test_df['outlet_city'].astype('category')

print("\nUpdated Data Types:")
print(test_df.dtypes)

numeric_cols = ['luxury_sales', 'fresh_sales', 'dry_sales']

def cap_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Cap values above the upper bound
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])

# Apply IQR capping to all sales columns
for col in ['luxury_sales', 'fresh_sales', 'dry_sales']:
    cap_outliers(test_df, col)

# Verify changes with a new boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(data=test_df[numeric_cols])
plt.title("Boxplot of Numeric Features (Train Data)")
plt.xticks(rotation=45)
plt.show()

"""Encoding"""

print("\nUnique outlet_city values in Test:", test_df['outlet_city'].nunique())
print(test_df['outlet_city'].value_counts())

"""Encoding using labelencoder"""

# Load the saved encoder
with open("/content/label_encoder_outlet.pkl", "rb") as f:
    le_outlet = pickle.load(f)

# Identify unknown cities
unknown_cities = set(test_df["outlet_city"]) - set(le_outlet.classes_)
print("\nUnknown Cities in Test Data:", unknown_cities)

# Dynamically extend encoding for new cities
new_labels = {city: len(le_outlet.classes_) + i for i, city in enumerate(unknown_cities)}

# Convert test dataset using the updated mapping
test_df["outlet_city_encoded"] = test_df["outlet_city"].map(
    lambda x: le_outlet.transform([x])[0] if x in le_outlet.classes_ else new_labels[x]
)

# Print the new mappings
print("\nUpdated Encoding for Test Data:")
print(new_labels)

# Save the updated encoder (optional)
with open("label_encoder_outlet.pkl", "wb") as f:
    pickle.dump(le_outlet, f)

"""Save the processed dataset"""

output_path = "/content/processed_Test_data.csv"
test_df.to_csv(output_path, index=False)