# -*- coding: utf-8 -*-
"""K_Nearest_Neighbors_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DL8MA2V5F01gV7SviioQh2EeAELUtkct
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

#Load the dataset
train_df = pd.read_csv('/content/Scaled_Train_data.csv')
test_df = pd.read_csv('/content/Scaled_Test_data.csv')

# Display the first few rows of the training and test datasets
print("Training Data:")
print(train_df.head())
print("\nTest Data:")
print(test_df.head())

#Assign input features and target variable
X = train_df.drop(columns=['cluster_catgeory'])  # Features
y = train_df['cluster_catgeory']  # Target variable

# Display the shape of the features and target variable
print("\nShape of Features (X):", X.shape)
print("Shape of Target (y):", y.shape)

#Split dataset into training and testing subsets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=14, stratify=y)

# Display the shape of the training and testing subsets
print("\nShape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

#Train initial KNN model (starting with k=9)
knn = KNeighborsClassifier(n_neighbors=9, metric='euclidean')
knn.fit(X_train, y_train)

#Evaluate the initial KNN model
y_train_pred = knn.predict(X_train)  # Predictions on training data
y_test_pred = knn.predict(X_test)  # Predictions on test data

# Compute accuracy scores
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

print("\nInitial Model Accuracy Scores:")
print(f"Train Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Print classification report
print("\nInitial Model Classification Report (Test Data):")
print(classification_report(y_test, y_test_pred))

# Display confusion matrix
cm = confusion_matrix(y_test, y_test_pred, labels=knn.classes_)
disp = ConfusionMatrixDisplay(cm, display_labels=knn.classes_)
disp.plot()
plt.title("Confusion Matrix - Initial Model")
plt.show()

# Finding the best k value (looping through k values from 1 to 40)
error = []
for i in range(1, 40):
    knn_loop = KNeighborsClassifier(n_neighbors=i)
    knn_loop.fit(X_train, y_train)
    pred_i = knn_loop.predict(X_test)
    error.append(np.mean(pred_i != y_test))

# Plot error rates for different k values
plt.figure(figsize=(12, 6))
plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',
         markerfacecolor='blue', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Error')
plt.show()

# Train final KNN model with optimal k (select the k with the lowest error)
optimal_k = error.index(min(error)) + 1
knn_final = KNeighborsClassifier(n_neighbors=optimal_k, metric='euclidean')
knn_final.fit(X_train, y_train)

# predictions
y_train_pred_final = knn_final.predict(X_train)
y_test_pred_final = knn_final.predict(X_test)

# Compute final accuracy scores
train_accuracy_final = accuracy_score(y_train, y_train_pred_final)
test_accuracy_final = accuracy_score(y_test, y_test_pred_final)

print("\nFinal Model Accuracy Scores:")
print(f"Train Accuracy: {train_accuracy_final:.4f}")
print(f"Test Accuracy: {test_accuracy_final:.4f}")

# Print classification report for final model
print("\nFinal Model Classification Report (Test Data):")
print(classification_report(y_test, y_test_pred_final))

# Display confusion matrix for optimized model
cm_final = confusion_matrix(y_test, y_test_pred_final, labels=knn_final.classes_)
disp_final = ConfusionMatrixDisplay(cm_final, display_labels=knn_final.classes_)
disp_final.plot()
plt.title("Confusion Matrix - Final Optimized Model")
plt.show()

# Reset index to keep row order
test_df = test_df.reset_index(drop=True)

# Rename column to match expected format
test_df.rename(columns={'customer_id': 'Customer_ID'}, inplace=True)

# Extract features (drop only Customer_ID)
X_test_final = test_df.drop(columns=['Customer_ID'])

# Predict using the trained model
y_test_pred = knn_final.predict(X_test_final)

# Create submission DataFrame
output_df = pd.DataFrame({
    'Customer_ID': test_df['Customer_ID'],
    'cluster_category': y_test_pred
})

# Save to CSV
output_df.to_csv('KNN_Predictions.csv', index=False)

# Show a preview
print(output_df.head())

from google.colab import files
files.download('/content/KNN_Predictions.csv')

# Display the first 10 rows of the predictions
print("\nFirst 10 Rows of Predictions for Test Dataset:")
print(output_df.head(10))

print(f"\nKNN Model training complete. Optimal k: {optimal_k}. Predictions saved as 'KNN_Predictions.csv'.")

print(f"Initial K: 9")
print(f"Optimal K from tuning: {optimal_k}")

print(f"Initial Train Accuracy: {train_accuracy:.4f}")
print(f"Initial Test Accuracy: {test_accuracy:.4f}")

print(f"Final Train Accuracy: {train_accuracy_final:.4f}")
print(f"Final Test Accuracy: {test_accuracy_final:.4f}")

#Feature Distribution Check
import seaborn as sns

plt.figure(figsize=(8, 6))
sns.histplot(y, bins=len(set(y)), kde=False)
plt.title("Distribution of Target Variable")
plt.xlabel("Cluster Category")
plt.ylabel("Count")
plt.show()