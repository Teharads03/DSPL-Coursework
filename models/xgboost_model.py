# -*- coding: utf-8 -*-
"""XGBoost Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1asisxXXR0xESIVBdNFag-VZuyV15k-Tn

Install & Import Libraries

Install XGBoost if not already installed
"""

!pip install xgboost

import pandas as pd
import numpy as np
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

"""Load and Prepare the Data"""

df = pd.read_csv("/content/Scaled_Train_data.csv")

""" Features and target"""

# Shift class labels from [1-6] to [0-5]
df["cluster_catgeory"] = df["cluster_catgeory"].astype(int) - df["cluster_catgeory"].min()

from sklearn.utils import shuffle

# Shuffle dataset to remove any ordering bias
df = shuffle(df, random_state=42).reset_index(drop=True)

X = df.drop(columns=["cluster_catgeory"])
y = df["cluster_catgeory"]

"""Split into train and validation sets"""

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)

"""Train the XGBoost Model"""

xgb_model = xgb.XGBClassifier(
    n_estimators=50,
    learning_rate=0.05,     # Lower learning rate for better generalization
    max_depth=3,            # Reduce tree depth to prevent overfitting
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=10,
    reg_alpha=5,
    objective="multi:softmax",
    num_class=6,
    random_state=42
)

xgb_model.fit(X_train, y_train)

""" Evaluate the Model"""

# Predict on validation data
y_pred_xgb = xgb_model.predict(X_val) + 1  # Shift back to [1-6] for readability
y_val_display = y_val + 1  # Also shift back to [1-6]

# Accuracy Score
print(f"XGBoost Accuracy: {accuracy_score(y_val_display, y_pred_xgb):.4f}")

# Classification Report
print("\nClassification Report:\n", classification_report(y_val_display, y_pred_xgb))

# Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(confusion_matrix(y_val_display, y_pred_xgb), annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("XGBoost - Confusion Matrix")
plt.show()

"""Feature Importance"""

# Plot feature importance
xgb.plot_importance(xgb_model, height=0.5, importance_type='gain')
plt.title("XGBoost - Feature Importance")
plt.tight_layout()
plt.show()

# Load test dataset
test_df = pd.read_csv("/content/Scaled_Test_data.csv")

# Ensure you're using the same features as during training
X_test_real = test_df[X.columns]  # X.columns comes from training

# Predict cluster categories (add +1 if you shifted during training)
cluster_predictions = xgb_model.predict(X_test_real) + 1  # back to [1-6]

# Create final submission DataFrame
submission_df = pd.DataFrame({
    'Customer_ID': test_df['Customer_ID'],
    'cluster_category': cluster_predictions
})

# Save to CSV
submission_df.to_csv("/content/XGBoost_Predictions.csv", index=False)

print(" Submission file saved as 'XGBoost_Predictions.csv'")
print(submission_df.head())

"""Predict on the Test Dataset"""

train_accuracy = accuracy_score(y_train, xgb_model.predict(X_train))
val_accuracy = accuracy_score(y_val, xgb_model.predict(X_val))

print(f"Train Accuracy: {train_accuracy:.4f}")
print(f"Validation Accuracy: {val_accuracy:.4f}")