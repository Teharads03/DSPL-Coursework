# -*- coding: utf-8 -*-
"""XGBoost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hnbiaEr2b-WmnLTW_MLqTKgQKEGYWXyQ

Install & Import Libraries

Install XGBoost if not already installed
"""

!pip install xgboost

import pandas as pd
import numpy as np
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

"""Load and Prepare the Data"""

df = pd.read_csv("/content/Scaled_Train_data.csv")

# Preview original class distribution
print("Original Cluster Distribution:")
print(df['cluster_catgeory'].value_counts())

"""Features and target"""

from sklearn.utils import shuffle

class_distribution = {
    1: 1.0,   # Keep 100% of Cluster 1
    2: 0.7,   # Keep 70% of Cluster 2
    3: 0.3,   # Keep 30% of Cluster 3
    4: 1.0,   # Keep 100% of Cluster 4
    5: 0.2,   # Keep 20% of Cluster 5
    6: 1.0    # Keep 100% of Cluster 6
}

"""Create the Imbalanced Dataset Using Sampling"""

# Apply sampling to each cluster based on defined proportions
imbalanced_df = pd.concat([
    group.sample(frac=class_distribution[label], random_state=42)
    for label, group in df.groupby('cluster_catgeory')
])

# Shuffle the imbalanced dataset
imbalanced_df = shuffle(imbalanced_df, random_state=42).reset_index(drop=True)

# Show new class distribution
print("New Cluster Distribution After Imbalance:")
print(imbalanced_df['cluster_catgeory'].value_counts())

"""Save the imbalanced dataset to a new CSV"""

imbalanced_df.to_csv("/content/Imbalanced_Train_data.csv", index=False)

print(" File saved as 'Imbalanced_Train_data.csv'")

"""Load and Split the Imbalanced Data"""

import xgboost as xgb
from sklearn.model_selection import train_test_split

# Load the imbalanced dataset
imbalanced_df = pd.read_csv("/content/Imbalanced_Train_data.csv")

# Define features and target
X = imbalanced_df.drop(columns=["cluster_catgeory"])
y = imbalanced_df["cluster_catgeory"]

# Split into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)

print(" Data split complete.")
print(f"Training size: {X_train.shape[0]} rows")
print(f"Validation size: {X_val.shape[0]} rows")

"""Train XGBoost and Evaluate Performance"""

# Shift labels: from [1–6] ➜ [0–5]
y_train = y_train - 1
y_val = y_val - 1

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Initialize and train the XGBoost model
xgb_model = xgb.XGBClassifier(
    n_estimators=50,
    learning_rate=0.05,
    max_depth=3,
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=10,
    reg_alpha=5,
    objective="multi:softmax",
    num_class=6,
    random_state=42
)

xgb_model.fit(X_train, y_train)

#Predict on validation set
y_pred = xgb_model.predict(X_val)

# Accuracy
accuracy = accuracy_score(y_val, y_pred)
print(f"\n Validation Accuracy: {accuracy:.4f}")

#  Classification report
print("\n Classification Report:")
print(classification_report(y_val, y_pred))

# Confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - XGBoost on Imbalanced Data")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.utils.class_weight import compute_sample_weight

#  Remove 'classes' argument to avoid error
sample_weights = compute_sample_weight(
    class_weight='balanced',
    y=y_train
)

"""Initialize XGBoost model"""

xgb_model_weighted = xgb.XGBClassifier(
    n_estimators=50,
    learning_rate=0.05,
    max_depth=3,
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=10,
    reg_alpha=5,
    objective="multi:softmax",
    num_class=6,
    random_state=42
)

#  Train with sample weights
xgb_model_weighted.fit(X_train, y_train, sample_weight=sample_weights)

#  Predict
y_pred_weighted = xgb_model_weighted.predict(X_val)

#  Accuracy and classification report
print(f"\n Weighted Model Validation Accuracy: {accuracy_score(y_val, y_pred_weighted):.4f}")
print("\n Classification Report (With Class Weights):")
print(classification_report(y_val, y_pred_weighted))

# Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(confusion_matrix(y_val, y_pred_weighted), annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - XGBoost (With Class Weights)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Apply SMOTE to Training Data"""

# Install imbalanced-learn if not already available
!pip install -q imbalanced-learn

from imblearn.over_sampling import SMOTE
import pandas as pd

# Apply SMOTE only on training set
sm = SMOTE(random_state=42)
X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)

# Show new class distribution
print("New Class Distribution After SMOTE:")
print(pd.Series(y_train_smote).value_counts())

"""Train XGBoost on SMOTE Data and Evaluate"""

import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Train XGBoost on SMOTE-balanced data
xgb_model_smote = xgb.XGBClassifier(
    n_estimators=50,
    learning_rate=0.05,
    max_depth=3,
    min_child_weight=3,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=10,
    reg_alpha=5,
    objective="multi:softmax",
    num_class=6,
    random_state=42
)

xgb_model_smote.fit(X_train_smote, y_train_smote)

"""Predict on original validation set"""

y_pred_smote = xgb_model_smote.predict(X_val)

# Metrics
print(f"\n SMOTE Model Validation Accuracy: {accuracy_score(y_val, y_pred_smote):.4f}")
print("\n Classification Report (With SMOTE):")
print(classification_report(y_val, y_pred_smote))

"""Confusion matrix"""

# Confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(confusion_matrix(y_val, y_pred_smote), annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - XGBoost (With SMOTE)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Feature Importance"""

importance = xgb_model_smote.get_booster().get_score(importance_type='gain')
importance_df = pd.DataFrame({
    'Feature': list(importance.keys()),
    'Importance': list(importance.values())
}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(8, 5))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')
plt.xlabel("Gain")
plt.title("Feature Importance")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

#  Load preprocessed and scaled test dataset
test_df = pd.read_csv("/content/Scaled_Test_data.csv")

#  Ensure same feature columns as training data
X_test_real = test_df[X_train.columns]  # Use original feature order

#  Predict cluster categories (shift back to [1–6] if model was trained on [0–5])
cluster_predictions = xgb_model_smote.predict(X_test_real) + 1

# Create submission DataFrame
submission_df = pd.DataFrame({
    'Customer_ID': test_df['Customer_ID'],
    'cluster_category': cluster_predictions
})

#  Save submission to CSV
submission_df.to_csv("/content/XGBoost_SMOTE_Predictions.csv", index=False)
print(" Submission file saved as 'XGBoost_SMOTE_Predictions.csv'")
print(submission_df.head())

#  Check model performance on train and validation sets
train_accuracy = accuracy_score(y_train, xgb_model_smote.predict(X_train))
val_accuracy = accuracy_score(y_val, xgb_model_smote.predict(X_val))

print(f"Train Accuracy (SMOTE model): {train_accuracy:.4f}")
print(f"Validation Accuracy (SMOTE model): {val_accuracy:.4f}")